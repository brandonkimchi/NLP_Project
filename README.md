To set up the project:
.\setup_env.ps1


# ğŸ¬ Text to Rating: Sentiment Classification and Rating Approximation in Movie Reviews

**Done By:** Kanapathi Vasudevan (s3751120) & Kim E-Shawn Brandon (s3747883)  
**University of Twente â€“ Project Group 6**

---

## ğŸ“˜ Overview

This repository contains a **multi-stage NLP pipeline** for **sentiment classification** and **rating approximation** of movie reviews.  
We integrate **classical ML**, **ensemble methods**, **deep learning**, and **unsupervised clustering** to predict review polarity and estimate numerical ratings from raw text.  
The project is based on the **IMDB movie review dataset** and demonstrates how hybrid NLP systems can generate interpretable and scalable sentiment ratings.

---

## ğŸ§© Key Features

- ğŸ”¤ **Text Cleaning Pipeline:** Unicode normalization, slang expansion, emoji conversion, lemmatization, stopword removal  
- ğŸ§® **Classical Models:** TF-IDF & Bag-of-Words with Logistic Regression and Naive Bayes  
- ğŸ§  **Ensemble Models:** Bagging, Boosting (AdaBoost / XGBoost), Hard & Soft Voting, and Improved Stacking  
- ğŸ¤– **Deep Learning Models:** CNN, Bi-LSTM, GloVe-LSTM, and fine-tuned **DistilBERT** transformer  
- ğŸ“Š **Unsupervised Clustering:** K-Means, Agglomerative, and DBSCAN over TF-IDF, GloVe, and Sentence-BERT embeddings  
- â­ **Rating Approximation:** Combines supervised sentiment probabilities with cluster-based weighting (macro, silhouette, uncertainty)  

---

## ğŸ§  Methodology

### 1. Dataset
**Source:** [IMDB Movie Review Dataset â€“ Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  
- 50,000 labeled reviews (25k positive / 25k negative)  
- Diverse text length and tone  

### 2. Preprocessing
- HTML tag removal, Unicode normalization  
- Emoji â†’ Text (`emoji.demojize`)  
- Slang expansion via curated dictionary  
- Tokenization + Lemmatization (spaCy, WordNet)  
- Stopword filtering and duplicate removal  

### 3. Model Training
| Category | Algorithms | Notes |
|-----------|-------------|-------|
| **Classical** | Logistic Regression, Naive Bayes | BoW / TF-IDF features |
| **Ensemble** | Random Forest, AdaBoost, XGBoost, Voting, Stacking | Meta-learning with TF-IDF features |
| **Deep Learning** | CNN, Bi-LSTM, GloVe + LSTM, DistilBERT | Contextual embeddings for semantic modeling |

Training split: **72% train / 8% validation / 20% test**

### 4. Evaluation
- Metrics: **Accuracy** & **Macro-F1**
- Tools: scikit-learn, TensorFlow/Keras, HuggingFace Transformers
- All experiments reproducible (`joblib` serialization, fixed seeds)

---

## ğŸ“Š Results Summary

| Model | Type | Accuracy |
|-------|------|-----------|
| TF-IDF + Logistic Regression | Classical | **0.8953** |
| Improved Stacking Ensemble | Ensemble | **0.907** |
| DistilBERT (Fine-Tuned) | Transformer | **0.9132** |

### ğŸ§® Rating Approximation 

| Method | Score (/10) |
|---------|-------------|
| Supervised Micro Average | 7.87 |
| Unsupervised Macro-by-Cluster | 7.87 |
| Uncertainty-Weighted | 8.25 |
| Silhouette-Weighted | 7.85 |

> The hybrid supervised + unsupervised method captures **sentiment strength distribution** rather than a simple average polarity.

---

## ğŸ§© Insights

- ğŸ† **DistilBERT** achieved the best accuracy overall.  
- âš–ï¸ **Improved stacking** nearly matched transformer performance with better interpretability.  
- ğŸ’¡ The pipeline generalizes well to informal datasets (e.g., YouTube comments).

---

## âš™ï¸ Tech Stack

- **Languages:** Python  
- **Libraries:** scikit-learn, TensorFlow, Keras, HuggingFace Transformers, Gensim, SpaCy, NLTK, XGBoost  
- **Utilities:** `joblib`, `emoji`, `matplotlib`, `pandas`, `numpy`  

---

## ğŸ§­ Repository Structure

